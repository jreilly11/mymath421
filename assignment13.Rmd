
---
output: 
  html_document:
  pdf_document: default
  word_document: default
title: "Assignment 13: Text Mining"
---

***How to do it?***: 

- Open the Rmarkdown file of this assignment ([link](assignment13.Rmd)) in Rstudio. 

- Right under each **question**, insert  a code chunk (you can use the hotkey `Ctrl + Alt + I` to add a code chunk) and code the solution for the question. 

- `Knit` the rmarkdown file (hotkey: `Ctrl + Alt + K`) to export an html.  

-  Publish the html file to your Githiub Page. 

***Submission***: Submit the link on Github of the assignment to Canvas

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```


[Sample Codes](text_mining_sample_codes2.html)

-------
```{r}
# Used lines below to install various packages
## install.packages("tidytext")
## install.packages("wordcloud")
## install.packages("textdata")

# Unable to install textrecipes as it was removed from CRAN repository (as of 11/13)
## install.packages("textrecipes")

# Loading libraries
library(dplyr)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(knitr)
library(tidytext)
library(wordcloud)
library(textdata)
```

### Netflix Data

**1.** Download the `netflix_titles` at this [link](../data/netflix_titles.csv).  Create a century variable taking two values:

    - '21' if the released_year is greater or equal to 2000, and
    
    - '20' otherwise. 
```{r}
# Reading in data
df <- read_csv("netflix_titles.csv")

df$century <- case_when(df$release_year >= 2000 ~ '21',
                        TRUE ~ '20')
```
    
**2. Word Frequency**    

  a. Convert the description to tokens, remove all the stop words. What are the top 10 frequent words of movies/TV Shows in the 20th century.  Plot the bar chart of the frequency of these words. 
```{r}
df %>% filter(century == '20') %>% 
  unnest_tokens(input = description, output = word) %>%
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>%
  group_by(type) %>% 
  slice_max(n, n=10) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = type)) %>%
  ggplot(aes(n, word, fill = type)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~type, scales = "free") +
  labs(x = "Frequency",
       y = NULL) +
  scale_y_reordered()
```

  b. What are the top 10 frequent words of movies/TV Shows in the 21st century. Plot the bar chart of the frequency of these words. Plot a side-by-side bar charts to compare the most frequent words by the two centuries. 
```{r}
df %>% filter(century == '21') %>% 
  unnest_tokens(input = description, output = word) %>%
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>%
  group_by(type) %>% 
  slice_max(n, n=10) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = type)) %>%
  ggplot(aes(n, word, fill = type)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~type, scales = "free") +
  labs(x = "Frequency",
       y = NULL) +
  scale_y_reordered()
```

**3. Word Cloud**

  a. Plot the word cloud of the words in the descriptions in the movies/TV Shows in the 20th century.
```{r}
colors <- brewer.pal(8,"Dark2")

df %>%
  filter(century == '20') %>%
  unnest_tokens(input = description, output = word) %>%
  anti_join(get_stopwords()) %>%
  count(type, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors = colors))
```

  b. Plot the word cloud of the words in the descriptions in the movies/TV Shows in the 21st century. 
```{r}
df %>%
  filter(century == '21') %>%
  unnest_tokens(input = description, output = word) %>%
  anti_join(get_stopwords()) %>%
  count(type, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors = colors))
```

**4. Sentiment Analysis**

  a. Is movies/TV Shows in the 21st century tends to be more positive than those in 20th century?  Use the sentiment analysis by `Bing` lexicons to address the question. 
```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(century, word, sort = TRUE) %>%
    group_by(century) %>% 
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(century) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(century, n, fill=sentiment))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

# According to sentiment analysis by 'Bing' lexicons, movies/TV shows in the 20th century tend to be more positive than those in the 21st century.
```
  
  b. Do sentiment analysis using `nrc` and `afinn` lexicons.  Give your comments on the results.
```{r}
# Sentiment analysis using 'nrc'
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(century, word, sort = TRUE) %>%
    group_by(century) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(century) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill=century))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

# Comments: For the most part, the relative frequency of 20th and 21st century movies/TV shows appears to be almost evenly split (50/50) regardless of sentiment type. However, there are small fluctuations, for example, 21st century moviews/TV shows appear to have a slightly higher relative frequency in the 'negative' and 'disgust' categories, while 21st century films/shows appear to have a higher relative frequency in the 'anticipation', 'fear', and 'joy' categories.

# Sentiment analysis using 'afinn'
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(century, word, sort = TRUE) %>%
    group_by(century) %>% 
    inner_join(get_sentiments("afinn")) %>%
    mutate(sentiment = value) %>% 
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(century) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill= century))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', x ='')

# Comments: It appears that the largest portion of films/shows, regardless of century, had a slightly negative sentiment (score of -2). Of these films/shows, the majority were from the 21st century. It also seems that the majority of films/shows viewed positiviely to very positively (scores of 3-5) came from the 20th century (had a larger proportion of films for all of these scores).
```

**5. Modeling**

  a. Use the description to predict if a movie/TV show is in 20th or 21st century. Give the accuracy and plot the confusion matrix table. 
```{r}
## Unable to run this cell because I can not install the textrecipes package (removed from CRAN repository as of 11/13/23). Also had to comment out so I could knit and submit this assignment.

####################################### Giving Accuracy ##################################
# library(caret)
# library(themis)
# library(textrecipes)
# 
# # Select data and set target 
# model_df <- df %>% 
#   mutate(target = century) %>% 
#   select(target, description) 
# 
# # Convert text data to numeric variables
# a <- recipe(target~description,
#        data = df) %>% 
#   step_tokenize(description) %>% 
#   step_tokenfilter(description, max_tokens = 50) %>% 
#   step_tfidf(description) %>% 
#   step_normalize(all_numeric_predictors()) %>% 
#   step_smote(target) %>% 
#   prep()
# model_df <- juice(a)
# 
# # Using Caret for modeling
# set.seed(2021)
# splitIndex <- createDataPartition(model_df$target, p = .7, 
#                                   list = FALSE)
# df_train <- model_df[ splitIndex,]
# df_test <- model_df[-splitIndex,]
# 
# forest_ranger <- train(target~., data=df_train, 
#                         method = "ranger")
# 
# pred <- predict(forest_ranger, df_test)
# 
# cm <- confusionMatrix(data = pred, reference = df_test$target)
# cm$overall[1]
# 
# ############################### Plotting Confusion Matrix ################################
# d = data.frame(pred = pred, obs = df_test$target)
# library(yardstick)
# d %>% conf_mat(pred, obs) %>% autoplot
```
  
  b. Create variable century2 taking three following values. (Hint: You can use the case_when function to do this)

    - `21` if released_year is greater or equal to 2000
    
    - `second_half_20`if released_year is greater than or equal to 1950 and less than 2000
    
    - `first_half_20` otherwise
```{r}
df$century2 <- case_when(df$release_year >= 2000 ~ '21',
                         df$release_year >= 1950 ~ 'second_half_20',
                         TRUE ~ 'first_half_20')
```
    
  Predict century2 using the descriptions. Give the accuracy and plot the confusion matrix table. (Notice that the codes for 8 should still work for this question)
```{r}
# Also can not run this cell because of textrecipes library. Had to comment out so I could knit and submit this assignment.

####################################### Giving Accuracy ##################################
# library(caret)
# library(themis)
# library(textrecipes)
# 
# # Select data and set target 
# model_df2 <- df %>% 
#   mutate(target = century2) %>% 
#   select(target, description) 
# 
# # Convert text data to numeric variables
# b <- recipe(target~description,
#        data = df) %>% 
#   step_tokenize(description) %>% 
#   step_tokenfilter(description, max_tokens = 50) %>% 
#   step_tfidf(description) %>% 
#   step_normalize(all_numeric_predictors()) %>% 
#   step_smote(target) %>% 
#   prep()
# model_df2 <- juice(b)
# 
# # Using Caret for modeling
# set.seed(2021)
# splitIndex <- createDataPartition(model_df2$target, p = .7, 
#                                   list = FALSE)
# df_train2 <- model_df2[ splitIndex,]
# df_test2 <- model_df2[-splitIndex,]
# 
# forest_ranger2 <- train(target~., data=df_train2, 
#                         method = "ranger")
# 
# pred2 <- predict(forest_ranger2, df_test2)
# 
# cm2 <- confusionMatrix(data = pred2, reference = df_test2$target)
# cm2$overall[1]
# 
# ############################### Plotting Confusion Matrix ################################
# d2 = data.frame(pred = pred2, obs = df_test2$target)
# library(yardstick)
# d2 %>% conf_mat(pred2, obs) %>% autoplot
```

**6.** Create another categorical variable from the data and do the following

    - Plot side-by-side word frequency by different categories of the newly created variable
    
    - Plot word clouds on different categories of the newly created variable
    
    - Do sentiment analysis to compare different categories of the newly created variable
    
    - Predict the newly created variable using the description. Give the accuracy and plot the confusion matrix table. 
```{r}
# Going to create a categorical variable that considers any films that are made only in the US as American, and all other films as Non-American (I am aware that some films were partially made in the US and partially made in other countries, however I would consider these films to be international rather than solely American)

# To do this, I must first create a subset of the df only containing films
movie_df <- df %>% filter(type == 'Movie')

movie_df$american <- case_when(movie_df$country == 'United States' ~ 'American',
                               TRUE ~ 'Non-American')
```
```{r}
# Word Frequency
movie_df %>%
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(american, word, sort = TRUE) %>% 
  group_by(american) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = american)) %>%
  ggplot(aes(n, word, fill = american)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~american, scales = "free") +
  labs(x = "Frequency",
       y = NULL)+
  scale_y_reordered()
```
```{r}
# Wordclouds
movie_df %>%
  filter(american == 'American') %>%
  unnest_tokens(input = description, output = word) %>%
  anti_join(get_stopwords()) %>%
  count(type, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors = colors))

movie_df %>%
  filter(american == 'Non-American') %>%
  unnest_tokens(input = description, output = word) %>%
  anti_join(get_stopwords()) %>%
  count(type, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors = colors))
```
```{r}
############################# Sentiment analyses ###################################
# Bing
movie_df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(american, word, sort = TRUE) %>%
    group_by(american) %>% 
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(american) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(american, n, fill=sentiment))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

# NRC
movie_df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(american, word, sort = TRUE) %>%
    group_by(american) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(american) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill=american))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

# Afinn
movie_df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(american, word, sort = TRUE) %>%
    group_by(american) %>% 
    inner_join(get_sentiments("afinn")) %>%
    mutate(sentiment = value) %>% 
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(american) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill= american))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', x ='')
```
```{r}
# Also can not run this cell due to textrecipes library (removed from CRAN repository as of 11/13/23). Also had to comment out so I could knit and submit this assignment.

####################################### Modeling #########################################

####################################### Giving Accuracy ##################################
# library(caret)
# library(themis)
# library(textrecipes)
# 
# # Select data and set target 
# model_df3 <- movie_df %>% 
#   mutate(target = american) %>% 
#   select(target, description) 
# 
# # Convert text data to numeric variables
# c <- recipe(target~description,
#        data = movie_df) %>% 
#   step_tokenize(description) %>% 
#   step_tokenfilter(description, max_tokens = 50) %>% 
#   step_tfidf(description) %>% 
#   step_normalize(all_numeric_predictors()) %>% 
#   step_smote(target) %>% 
#   prep()
# model_df3 <- juice(c)
# 
# # Using Caret for modeling
# set.seed(2021)
# splitIndex <- createDataPartition(model_df3$target, p = .7, 
#                                   list = FALSE)
# df_train3 <- model_df3[ splitIndex,]
# df_test3 <- model_df3[-splitIndex,]
# 
# forest_ranger3 <- train(target~., data=df_train3, 
#                         method = "ranger")
# 
# pred3 <- predict(forest_ranger3, df_test3)
# 
# cm3 <- confusionMatrix(data = pred3, reference = df_test3$target)
# cm3$overall[1]
# 
# ############################### Plotting Confusion Matrix ################################
# d3 = data.frame(pred = pred3, obs = df_test3$target)
# library(yardstick)
# d3 %>% conf_mat(pred3, obs) %>% autoplot
```
    
-------

### Animal Reviews Data (Optional)

We will study the Animal Crossing Data at [Kaggle](https://www.kaggle.com/jessemostipak/animal-crossing). The data file is `user_review`

**7.**  Download the animal reviews data at this [link](../data/user_reviews.tsv).  Read the data using `read_tsv()` function.
```{r}
animal_df <- read_tsv('user_reviews.tsv')
```

**8.** Create a `rating` variable taking value `good` if the grade is greater than 7 and `bad` otherwise. 
```{r}
animal_df$rating <- case_when(animal_df$grade > 7 ~ 'good',
                              TRUE ~ 'bad')
```

**9.** Do the follows. Notice that the text information is in the `text` variable. 

    - Plot side-by-side word frequency by different categories of the `rating` variable
    
    - Plot word clouds on different categories of the `rating` variable
    
    - Do sentiment analysis to compare different categories of the `rating` variable
    
    - Predict the rating using the reviews (`text` variable). Give the accuracy and plot the confusion matrix table.
```{r}
# Word frequency
animal_df %>%
  unnest_tokens(input = text, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(rating, word, sort = TRUE) %>% 
  group_by(rating) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = rating)) %>%
  ggplot(aes(n, word, fill = rating)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~rating, scales = "free") +
  labs(x = "Frequency",
       y = NULL)+
  scale_y_reordered() 
```
```{r}
# Word clouds
animal_df %>%
  filter(rating == 'good') %>% 
  unnest_tokens(input = text, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(rating, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=colors))

animal_df %>%
  filter(rating == 'bad') %>% 
  unnest_tokens(input = text, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(rating, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=colors))
```
```{r}
############################## Sentiment Analysis ########################################

# Bing
animal_df %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(rating, word, sort = TRUE) %>%
    group_by(rating) %>% 
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(rating) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(rating, n, fill=sentiment))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

# NRC
animal_df %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(rating, word, sort = TRUE) %>%
    group_by(rating) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(rating) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill=rating))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

# Afinn
animal_df %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(rating, word, sort = TRUE) %>%
    group_by(rating) %>% 
    inner_join(get_sentiments("afinn")) %>%
    mutate(sentiment = value) %>% 
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(rating) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill= rating))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', x ='')
```
```{r}
# Also can not run this cell due to textrecipes library (removed from CRAN repository as of 11/13/23). Also had to comment out so I could knit and submit this assignment.

####################################### Modeling #########################################

####################################### Giving Accuracy ##################################
# library(caret)
# library(themis)
# library(textrecipes)
# 
# # Select data and set target 
# model_df4 <- animal_df %>% 
#   mutate(target = rating) %>% 
#   select(target, text) 
# 
# # Convert text data to numeric variables
# d <- recipe(target~text,
#        data = animal_df) %>% 
#   step_tokenize(text) %>% 
#   step_tokenfilter(text, max_tokens = 50) %>% 
#   step_tfidf(text) %>% 
#   step_normalize(all_numeric_predictors()) %>% 
#   step_smote(target) %>% 
#   prep()
# model_df4 <- juice(d)
# 
# # Using Caret for modeling
# set.seed(2021)
# splitIndex <- createDataPartition(model_df4$target, p = .7, 
#                                   list = FALSE)
# df_train4 <- model_df4[ splitIndex,]
# df_test4 <- model_df4[-splitIndex,]
# 
# forest_ranger4 <- train(target~., data=df_train4, 
#                         method = "ranger")
# 
# pred4 <- predict(forest_ranger4, df_test4)
# 
# cm4 <- confusionMatrix(data = pred4, reference = df_test4$target)
# cm4$overall[1]
# 
# ############################### Plotting Confusion Matrix ################################
# d4 = data.frame(pred = pred4, obs = df_test4$target)
# library(yardstick)
# d4 %>% conf_mat(pred4, obs) %>% autoplot
```

